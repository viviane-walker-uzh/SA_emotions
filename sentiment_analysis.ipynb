{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3hLoBsJIJBQ"
      },
      "source": [
        "**This is the python code for the Seminar Project SENTIMENT ANALYSIS AMONG THREE SPIRITUAL EMOTIONS by Elma Pusharaja and Viviane Walker**\n",
        "\n",
        "\n",
        "Seminar: Language Technology for Social Media Analysis\n",
        "\n",
        "Offered by: The Departement of Computational Linguistics and the Departement of Communication Science and Media Research\n",
        "\n",
        "Author: Viviane Walker\n",
        "\n",
        "Date: Jan 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaZv_geHydj0"
      },
      "source": [
        "# Essential installs and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JBEh9znxQh6o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00LZwYlgwsI0"
      },
      "source": [
        "# Data (Preprocessing)\n",
        "- import dictionary and filter relevant keywords\n",
        "- keyword translations (manual)\n",
        "- data collection with R\n",
        "- data preprocessing for the sentiment analyses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSzpIB4bbQnM"
      },
      "source": [
        "## Spiritual Emotions Keywords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E2vXc1fisX7p"
      },
      "outputs": [],
      "source": [
        "dest = open('pone.0239050.s003.dic', 'rb').read()\n",
        "dest_decoded = dest.decode('latin1')\n",
        "dest_final = dest_decoded.split(\"\\r\\n\")\n",
        "dest_final = [ele.split(\"\\t\") for ele in dest_final]\n",
        "\n",
        "# checkpoint:\n",
        "#print(dest)\n",
        "#print(dest_decoded)\n",
        "#print(dest_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erWz8ljfshDa",
        "outputId": "b7b08882-5d89-4435-e808-7f253dec66c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "59\n",
            "61\n",
            "97\n",
            "['admir*', 'aesthetic', 'amazing', 'amazed', 'astonish*', 'awake*', 'awe*', 'beaut*', 'breathtak*', 'brilliant', 'captivat*', 'christ*', 'courage*', 'curiosity', 'delight*', 'devot*', 'divin*', 'enchant*', 'engag*', 'enlight*', 'enliven*', 'esteem*', 'evok*', 'excit*', 'fascinat*', 'god*', 'goosebump*', 'grand*', 'grat*', 'great', 'humility', 'incomprehens*', 'inspir*', 'interest*', 'invigorat*', 'lord', 'lov*', 'magnific*', 'majest*', 'mighty', 'mind-bending', 'mind-blowing', 'mind-boggling', 'mirac*', 'myst*', 'power*', 'prais*', 'raptur*', 'remark*', 'respect*', 'rever*', 'shock*', 'specta*', 'spirit*', 'stimulat*', 'stunning', 'wisdom', 'wonder*', 'worship*']\n",
            "['acknowledg*', 'admir*', 'affection', 'almighty', 'appreciat*', 'belove*', 'benevolen*', 'bestow*', 'bless*', 'charit*', 'christ*', 'compassion*', 'content*', 'credit', 'dedicat*', 'delight*', 'devot*', 'donat*', 'earn*', 'esteem*', 'favor*', 'forgiv*', 'fulfill*', 'genero*', 'gift', 'goodness', 'grace', 'grat*', 'happiness', 'happy', 'heart*', 'help*', 'honesty', 'honor*', 'hospitality', 'humility', 'indebt*', 'kind*', 'loyal*', 'mirac*', 'oblig*', 'pleas*', 'prais*', 'pray*', 'recogn*', 'recompense', 'repay', 'rescue*', 'respect*', 'responsive*', 'revel', 'rever*', 'sacrific*', 'satisf*', 'selfless*', 'sincer*', 'spirit*', 'thank*', 'thrill*', 'tribute', 'virtu*']\n",
            "['achiev*', 'ambition', 'anticipat*', 'aspir*', 'assum*', 'assur*', 'at ease', 'auspicious', 'await', 'belie*', 'bright', 'buoyan*', 'cheer*', 'cherish', 'comfort*', 'compassion*', 'confiden*', 'congratulat*', 'contemplat*', 'content*', 'count on', 'curable', 'deliverance', 'dependence', 'desir*', 'doubtless', 'dream', 'eager', 'elat*', 'elevat*', 'embolden*', 'encourag*', 'endur*', 'enthusias*', 'envision*', 'expect*', 'faith*', 'favor*', 'foresee', 'foretell', 'fortitude', 'fortun*', 'forward', 'freedom', 'future', 'gain', 'glory', 'goal', 'golden', 'good', 'grow*', 'heart*', 'hope*', 'ideal*', 'idyll*', 'incentiv*', 'inspir*', 'instill*', 'learn*', 'lighthearted', 'limitless', 'newborn', 'opportunit*', 'optimis*', 'overcom*', 'paradise', 'perfect*', 'persever*', 'positiv*', 'promis*', 'prophesy', 'propitious', 'prospect*', 'purpos*', 'reassur*', 'redeem', 'redempt*', 'regain*', 'reliance', 'restor*', 'rever*', 'reward*', 'sanguine*', 'satisf*', 'secur*', 'seren*', 'sincer*', 'survive', 'triumph', 'trust*', 'upbeat', 'uplift*', 'utopia*', 'virtu*', 'wholehearted', 'wish', 'youth*']\n"
          ]
        }
      ],
      "source": [
        "#create lists with emotion keywords\n",
        "\n",
        "key_awe = []       #category 02\n",
        "key_gratitude = [] #category 03\n",
        "key_hope = []      #category 06\n",
        "\n",
        "for i in range(9, len(dest_final)): # start: 9 because of category legend in the header of the dic file\n",
        "\n",
        "  word = dest_final[i][0]\n",
        "\n",
        "  for ii in range(1, len(dest_final[i])): #['cherish', '05', '06']\n",
        "    if dest_final[i][ii] == \"02\":\n",
        "      key_awe.append(word)\n",
        "    elif dest_final[i][ii] == \"03\":\n",
        "      key_gratitude.append(word)\n",
        "    elif dest_final[i][ii] == \"06\":\n",
        "      key_hope.append(word)\n",
        "    else:\n",
        "      continue # skip other emotions keywords\n",
        "\n",
        "\n",
        "print(len(key_awe))       #59\n",
        "print(len(key_gratitude)) #61\n",
        "print(len(key_hope))      #97\n",
        "print(key_awe)\n",
        "print(key_gratitude)\n",
        "print(key_hope)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I copied these lists to a word document in order to translate them manually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8rhRpFmySEG"
      },
      "source": [
        "## Data Collection\n",
        "\n",
        "This is done with R which supports keyword based reddit post extraction thanks to the RedditExtractoR package by Ivan Rivera.\n",
        "\n",
        "TODO link file in the github repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Datasets import\n",
        "I import here the three datasets created in the R file with the three translated keyword lists from above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "awe_df_all = pd.read_csv(\"data/awe_reddit_data.csv\")\n",
        "gratitude_df_all = pd.read_csv(\"data/gratitude_reddit_data.csv\")\n",
        "hope_df_all = pd.read_csv(\"data/hope_reddit_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EseGldx7RaIO"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "random sampling 2000 rows from each dataframe\n",
        "\n",
        "\n",
        "2000 because it is the size of the smallest dataset and the goal is to compare overall sentiment of all three dataset hence the datasets must be of equal number of rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4021\n",
            "3865\n",
            "7691\n"
          ]
        }
      ],
      "source": [
        "# before\n",
        "print(len(awe_df_all))\n",
        "print(len(gratitude_df_all))\n",
        "print(len(hope_df_all))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NaNs in awe dataset:  1334\n"
          ]
        }
      ],
      "source": [
        "# check for NaNs in title and text column\n",
        "print(\"NaNs in awe dataset: \", awe_df_all[\"text\"].isna().sum())\n",
        "awe_df_all_nonans = awe_df_all.dropna(subset=[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "awe_df = awe_df_all_nonans.sample(2000, random_state=13)\n",
        "# gratitude_df = gratitude_df_all_nonas.sample(2000, random_state=13)\n",
        "# hope_df = hope_df_all_nonans.sample(2000, random_state=13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n"
          ]
        }
      ],
      "source": [
        "# checkpoint, should print 2000,  3 times\n",
        "print(len(awe_df))\n",
        "# print(len(gratitude_df))\n",
        "# print(len(hope_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogLjYhMtvzsH"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKv20SICqfwo"
      },
      "source": [
        "## German Sentiment Analysis\n",
        "\n",
        "from the repo: https://github.com/oliverguhr/german-sentiment-lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wHo1YKojqjWU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/vi/Library/CloudStorage/OneDrive-Personal/UZH/HS23/Lang Tech for Social Media Analysis/sentiment analysis project/.venv/sa_emotions/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/vi/Library/CloudStorage/OneDrive-Personal/UZH/HS23/Lang Tech for Social Media Analysis/sentiment analysis project/.venv/sa_emotions/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from germansentiment import SentimentModel\n",
        "model = SentimentModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 32  # Choose an appropriate batch size\n",
        "\n",
        "# Get the number of batches\n",
        "num_batches = len(awe_df) // batch_size\n",
        "\n",
        "# Process each batch\n",
        "for i in range(num_batches):\n",
        "    start_idx = i * batch_size\n",
        "    end_idx = (i + 1) * batch_size\n",
        "\n",
        "    # Extract the batch of texts\n",
        "    batch_texts = awe_df[\"text\"].iloc[start_idx:end_idx]\n",
        "\n",
        "    # Perform sentiment prediction on the batch\n",
        "    batch_classes, batch_probabilities = model.predict_sentiment(batch_texts, output_probabilities=True)\n",
        "\n",
        "    # Process the results as needed (e.g., store or analyze them)\n",
        "\n",
        "# Process the remaining texts (if any) that don't fit exactly in a batch\n",
        "remaining_texts = awe_df[\"text\"].iloc[num_batches * batch_size:]\n",
        "if not remaining_texts.empty:\n",
        "    remaining_classes, remaining_probabilities = model.predict_sentiment(remaining_texts, output_probabilities=True)\n",
        "\n",
        "    # Process the results for the remaining texts as needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# awe\n",
        "classes, probabilities = model.predict_sentiment(awe_df[\"text\"], output_probabilities = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "awe_df[\"class\"] = classes\n",
        "awe_df[\"pos_prob\"] = [ele[0][1] for ele in probabilities]\n",
        "awe_df[\"neg_prob\"] = [ele[1][1] for ele in probabilities]\n",
        "awe_df[\"neut_prob\"] = [ele[2][1] for ele in probabilities]\n",
        "\n",
        "# checkpoint\n",
        "print(awe_df[[\"class\", \"pos_prob\", \"neg_prob\", \"neut_prob\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "c = 0\n",
        "res_classes = []\n",
        "class_overall = []\n",
        "title_lengths = []\n",
        "\n",
        "for row in awe_df[\"text\"]:\n",
        "    if c == 21:\n",
        "        break\n",
        "    print(repr(row))\n",
        "    classes, probabilities = model.predict_sentiment([row], output_probabilities = True)\n",
        "    dic_classes = Counter(classes)\n",
        "    res_classes.append(dic_classes)\n",
        "    title_lengths.append(len(row.split(\" \"))) # word count\n",
        "    class_overall.append(max(dic_classes, key=dic_classes.get))\n",
        "    c+= 1\n",
        "\n",
        "awe_df[\"classes_distribution\"] = res_classes\n",
        "awe_df[\"class_overall\"] = class_overall\n",
        "awe_df[\"title_length\"] = title_lengths\n",
        "print(awe_df[[\"title_length\", \"classes_distribution\", \"class_overall\"]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "awe_df[\"class_overall\"].value_counts().plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
